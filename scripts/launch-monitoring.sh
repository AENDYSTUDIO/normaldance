#!/bin/bash

# NormalDance Launch and Monitoring Script
# This script handles the final launch preparation and ongoing monitoring

set -e

# Configuration
NAMESPACE="production"
CLUSTER_NAME="normaldance-prod"
REGION="us-east-1"
MONITORING_DURATION="30d"
HEALTH_CHECK_INTERVAL="30s"
METRICS_RETENTION="30d"
ALERT_EMAILS="admin@normaldance.app,devops@normaldance.app"
SLACK_WEBHOOK="${SLACK_WEBHOOK:-}"
GRAFANA_URL="${GRAFANA_URL:-}"
PROMETHEUS_URL="${PROMETHEUS_URL:-}"
DATADOG_API_KEY="${DATADOG_API_KEY:-}"

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
NC='\033[0m' # No Color

# Logging function
log() {
    echo -e "${GREEN}[INFO]${NC} $1"
}

warn() {
    echo -e "${YELLOW}[WARN]${NC} $1"
}

error() {
    echo -e "${RED}[ERROR]${NC} $1"
    exit 1
}

# Check prerequisites
check_prerequisites() {
    log "Checking prerequisites..."
    
    # Check if required tools are installed
    if ! command -v kubectl &> /dev/null; then
        error "kubectl is not installed"
    fi
    
    if ! command -v helm &> /dev/null; then
        error "helm is not installed"
    fi
    
    if ! command -v aws &> /dev/null; then
        error "aws-cli is not installed"
    fi
    
    if ! command -v curl &> /dev/null; then
        error "curl is not installed"
    fi
    
    if ! command -v jq &> /dev/null; then
        error "jq is not installed"
    fi
    
    log "All prerequisites are installed"
}

# Create monitoring directory
create_monitoring_directory() {
    log "Creating monitoring directory..."
    
    # Create monitoring directory
    mkdir -p "monitoring/launch"
    mkdir -p "monitoring/alerts"
    mkdir -p "monitoring/dashboards"
    mkdir -p "monitoring/scripts"
    mkdir -p "monitoring/logs"
    
    log "Monitoring directory created"
}

# Set up monitoring infrastructure
setup_monitoring_infrastructure() {
    log "Setting up monitoring infrastructure..."
    
    # Create Prometheus configuration
    cat > "monitoring/prometheus-config.yaml" << EOF
global:
  scrape_interval: 15s
  evaluation_interval: 15s

rule_files:
  - "alert_rules.yml"

alerting:
  alertmanagers:
    - static_configs:
        - targets:
          - alertmanager:9093

scrape_configs:
  - job_name: 'normaldance-api'
    static_configs:
      - targets: ['normaldance-api:3000']
    metrics_path: '/metrics'
    scrape_interval: 15s
    
  - job_name: 'normaldance-web'
    static_configs:
      - targets: ['normaldance-web:3000']
    metrics_path: '/metrics'
    scrape_interval: 15s
    
  - job_name: 'normaldance-db'
    static_configs:
      - targets: ['normaldance-db:5432']
    metrics_path: '/metrics'
    scrape_interval: 15s
    
  - job_name: 'normaldance-redis'
    static_configs:
      - targets: ['normaldance-redis:6379']
    metrics_path: '/metrics'
    scrape_interval: 15s
    
  - job_name: 'normaldance-ipfs'
    static_configs:
      - targets: ['normaldance-ipfs:5001']
    metrics_path: '/metrics'
    scrape_interval: 15s
    
  - job_name: 'normaldance-solana'
    static_configs:
      - targets: ['normaldance-solana:8899']
    metrics_path: '/metrics'
    scrape_interval: 15s
    
  - job_name: 'node-exporter'
    static_configs:
      - targets: ['node-exporter:9100']
    metrics_path: '/metrics'
    scrape_interval: 15s
    
  - job_name: 'cadvisor'
    static_configs:
      - targets: ['cadvisor:8080']
    metrics_path: '/metrics'
    scrape_interval: 15s
    
  - job_name: 'blackbox'
    static_configs:
      - targets: ['blackbox-exporter:9115']
    metrics_path: '/probe'
    scrape_interval: 15s
    params:
      module: [http_2xx]
EOF
    
    # Create alert rules
    cat > "monitoring/alert_rules.yml" << EOF
groups:
  - name: normaldance-alerts
    rules:
      - alert: HighErrorRate
        expr: rate(http_requests_total{status=~"5.."}[5m]) > 0.1
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "High error rate detected"
          description: "Error rate is {{ \$value }} errors per second"
          
      - alert: HighResponseTime
        expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m])) > 1
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High response time detected"
          description: "95th percentile response time is {{ \$value }} seconds"
          
      - alert: LowDatabaseConnections
        expr: pg_stat_database_numbackends < 5
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Low database connections"
          description: "Database connections are low: {{ \$value }}"
          
      - alert: HighMemoryUsage
        expr: (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 80
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "High memory usage"
          description: "Memory usage is at {{ \$value }}%"
          
      - alert: HighCPUUsage
        expr: 100 - (avg by(instance) (rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 80
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High CPU usage"
          description: "CPU usage is at {{ \$value }}%"
          
      - alert: DiskSpaceLow
        expr: (1 - (node_filesystem_avail_bytes / node_filesystem_size_bytes)) * 100 > 85
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Low disk space"
          description: "Disk space usage is at {{ \$value }}%"
          
      - alert: ServiceDown
        expr: up == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Service is down"
          description: "Service {{ \$labels.instance }} is down"
          
      - alert: HighLatency
        expr: rate(http_request_duration_seconds_sum[5m]) / rate(http_request_duration_seconds_count[5m]) > 2
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High latency detected"
          description: "Average latency is {{ \$value }} seconds"
EOF
    
    # Create Grafana dashboard
    cat > "monitoring/dashboards/normaldance-dashboard.json" << EOF
{
  "dashboard": {
    "id": null,
    "title": "NormalDance Platform Overview",
    "tags": ["normaldance", "platform"],
    "timezone": "browser",
    "panels": [
      {
        "id": 1,
        "title": "HTTP Requests",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(http_requests_total[5m])",
            "legendFormat": "{{method}} {{status}}"
          }
        ],
        "gridPos": {"h": 8, "w": 12, "x": 0, "y": 0}
      },
      {
        "id": 2,
        "title": "Response Time",
        "type": "graph",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))",
            "legendFormat": "95th percentile"
          }
        ],
        "gridPos": {"h": 8, "w": 12, "x": 12, "y": 0}
      },
      {
        "id": 3,
        "title": "Database Connections",
        "type": "singlestat",
        "targets": [
          {
            "expr": "pg_stat_database_numbackends",
            "legendFormat": "Connections"
          }
        ],
        "gridPos": {"h": 8, "w": 6, "x": 0, "y": 8}
      },
      {
        "id": 4,
        "title": "Memory Usage",
        "type": "singlestat",
        "targets": [
          {
            "expr": "(1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100",
            "legendFormat": "Memory Usage"
          }
        ],
        "gridPos": {"h": 8, "w": 6, "x": 6, "y": 8}
      },
      {
        "id": 5,
        "title": "CPU Usage",
        "type": "singlestat",
        "targets": [
          {
            "expr": "100 - (avg by(instance) (rate(node_cpu_seconds_total{mode=\"idle\"}[5m])) * 100)",
            "legendFormat": "CPU Usage"
          }
        ],
        "gridPos": {"h": 8, "w": 6, "x": 12, "y": 8}
      },
      {
        "id": 6,
        "title": "Active Users",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(active_users_total[5m])",
            "legendFormat": "Active Users"
          }
        ],
        "gridPos": {"h": 8, "w": 12, "x": 0, "y": 16}
      },
      {
        "id": 7,
        "title": "Track Plays",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(track_plays_total[5m])",
            "legendFormat": "Track Plays"
          }
        ],
        "gridPos": {"h": 8, "w": 12, "x": 12, "y": 16}
      }
    ],
    "time": {"from": "now-1h", "to": "now"},
    "timepicker": {
      "refresh_intervals": ["5s", "15s", "30s", "1m", "5m", "15m", "30m", "1h", "2h", "6h"]
    }
  }
}
EOF
    
    # Create alert manager configuration
    cat > "monitoring/alertmanager-config.yaml" << EOF
global:
  smtp_smarthost: 'localhost:587'
  smtp_from: 'alerts@normaldance.app'
  smtp_auth_username: 'alerts@normaldance.app'
  smtp_auth_password: 'your-smtp-password'

route:
  group_by: ['alertname']
  group_wait: 10s
  group_interval: 10s
  repeat_interval: 1h
  receiver: 'web.hook'

receivers:
  - name: 'web.hook'
    email_configs:
      - to: '$ALERT_EMAILS'
        subject: 'NormalDance Alert: {{ .GroupLabels.alertname }}'
        body: |
          {{ range .Alerts }}
          Alert: {{ .Annotations.summary }}
          Description: {{ .Annotations.description }}
          Status: {{ .Status }}
          Labels: {{ .Labels }}
          {{ end }}
    webhook_configs:
      - url: '$SLACK_WEBHOOK'
        send_resolved: true
EOF
    
    log "Monitoring infrastructure setup completed"
}

# Set up health checks
setup_health_checks() {
    log "Setting up health checks..."
    
    # Create health check script
    cat > "monitoring/scripts/health-check.sh" << EOF
#!/bin/bash

# NormalDance Health Check Script
# This script performs comprehensive health checks

set -e

# Configuration
HEALTH_CHECK_DIR="monitoring/health"
LOG_DIR="monitoring/logs"
API_URL="https://api.normaldance.com"
WEB_URL="https://normaldance.app"
CHECK_INTERVAL=30
MAX_RETRIES=3

# Colors for output
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
RED='\033[0;31m'
NC='\033[0m'

log() {
    echo -e "\${GREEN}[INFO]\${NC} \$1"
}

warn() {
    echo -e "\${YELLOW}[WARN]\${NC} \$1"
}

error() {
    echo -e "\${RED}[ERROR]\${NC} \$1"
}

# Create health check directory
mkdir -p "\$HEALTH_CHECK_DIR"
mkdir -p "\$LOG_DIR"

# Function to check API health
check_api_health() {
    local endpoint="\$1"
    local max_retries=\$2
    local retry_count=0
    
    while [ \$retry_count -lt \$max_retries ]; do
        local response=\$(curl -s -o /dev/null -w "%{http_code}" "\$API_URL/\$endpoint")
        
        if [ "\$response" -eq 200 ]; then
            log "API endpoint \$endpoint is healthy"
            return 0
        else
            warn "API endpoint \$endpoint returned \$response (attempt \$((retry_count + 1))/\$max_retries)"
            retry_count=\$((retry_count + 1))
            sleep 5
        fi
    done
    
    error "API endpoint \$endpoint is unhealthy after \$max_retries attempts"
    return 1
}

# Function to check web application health
check_web_health() {
    local max_retries=\$1
    local retry_count=0
    
    while [ \$retry_count -lt \$max_retries ]; do
        local response=\$(curl -s -o /dev/null -w "%{http_code}" "\$WEB_URL")
        
        if [ "\$response" -eq 200 ]; then
            log "Web application is healthy"
            return 0
        else
            warn "Web application returned \$response (attempt \$((retry_count + 1))/\$max_retries)"
            retry_count=\$((retry_count + 1))
            sleep 5
        fi
    done
    
    error "Web application is unhealthy after \$max_retries attempts"
    return 1
}

# Function to check database health
check_database_health() {
    local max_retries=\$1
    local retry_count=0
    
    while [ \$retry_count -lt \$max_retries ]; do
        local response=\$(curl -s "\$API_URL/health/database" | jq -r '.status')
        
        if [ "\$response" = "healthy" ]; then
            log "Database is healthy"
            return 0
        else
            warn "Database is \$response (attempt \$((retry_count + 1))/\$max_retries)"
            retry_count=\$((retry_count + 1))
            sleep 5
        fi
    done
    
    error "Database is unhealthy after \$max_retries attempts"
    return 1
}

# Function to check external services
check_external_services() {
    local services=("https://api.spotify.com" "https://api.apple.com" "https://ipfs.io")
    
    for service in "\${services[@]}"; do
        local response=\$(curl -s -o /dev/null -w "%{http_code}" "\$service")
        
        if [ "\$response" -eq 200 ]; then
            log "External service \$service is accessible"
        else
            warn "External service \$service returned \$response"
        fi
    done
}

# Function to check system resources
check_system_resources() {
    local cpu_usage=\$(top -bn1 | grep "Cpu(s)" | awk '{print \$2}' | cut -d'%' -f1)
    local memory_usage=\$(free | grep Mem | awk '{printf "%.2f", \$3/\$2 * 100.0}')
    local disk_usage=\$(df -h / | awk 'NR==2{print \$5}' | cut -d'%' -f1)
    
    log "System Resources:"
    log "  CPU Usage: \$cpu_usage%"
    log "  Memory Usage: \$memory_usage%"
    log "  Disk Usage: \$disk_usage%"
    
    # Check if resources are within acceptable limits
    if (( \$(echo "\$cpu_usage > 80" | bc -l) )); then
        warn "High CPU usage detected: \$cpu_usage%"
    fi
    
    if (( \$(echo "\$memory_usage > 80" | bc -l) )); then
        warn "High memory usage detected: \$memory_usage%"
    fi
    
    if [ "\$disk_usage" -gt 80 ]; then
        warn "High disk usage detected: \$disk_usage%"
    fi
}

# Function to generate health report
generate_health_report() {
    local timestamp=\$(date +%Y%m%d_%H%M%S)
    local report_file="\$HEALTH_CHECK_DIR/health-report-\$timestamp.json"
    
    cat > "\$report_file" << JSON
{
  "timestamp": "\$(date -Iseconds)",
  "api_health": {
    "status": "\$(curl -s "\$API_URL/health" | jq -r '.status')",
    "uptime": "\$(curl -s "\$API_URL/health" | jq -r '.uptime')",
    "version": "\$(curl -s "\$API_URL/health" | jq -r '.version')"
  },
  "database_health": {
    "status": "\$(curl -s "\$API_URL/health/database" | jq -r '.status')",
    "connections": "\$(curl -s "\$API_URL/health/database" | jq -r '.connections')",
    "queries_per_second": "\$(curl -s "\$API_URL/health/database" | jq -r '.queries_per_second')"
  },
  "system_resources": {
    "cpu_usage": "\$(top -bn1 | grep "Cpu(s)" | awk '{print \$2}' | cut -d'%' -f1)",
    "memory_usage": "\$(free | grep Mem | awk '{printf "%.2f", \$3/\$2 * 100.0}')",
    "disk_usage": "\$(df -h / | awk 'NR==2{print \$5}' | cut -d'%' -f1)"
  },
  "external_services": {
    "spotify": "\$(curl -s -o /dev/null -w "%{http_code}" "https://api.spotify.com")",
    "apple": "\$(curl -s -o /dev/null -w "%{http_code}" "https://api.apple.com")",
    "ipfs": "\$(curl -s -o /dev/null -w "%{http_code}" "https://ipfs.io")"
  }
}
JSON
    
    log "Health report generated: \$report_file"
}

# Main health check function
perform_health_checks() {
    log "Starting comprehensive health checks..."
    
    # Check API health
    check_api_health "health" \$MAX_RETRIES
    check_api_health "tracks" \$MAX_RETRIES
    check_api_health "users" \$MAX_RETRIES
    check_api_health "wallet" \$MAX_RETRIES
    
    # Check web application health
    check_web_health \$MAX_RETRIES
    
    # Check database health
    check_database_health \$MAX_RETRIES
    
    # Check external services
    check_external_services
    
    # Check system resources
    check_system_resources
    
    # Generate health report
    generate_health_report
    
    log "Health checks completed successfully"
}

# Continuous monitoring function
continuous_monitoring() {
    log "Starting continuous monitoring..."
    
    while true; do
        perform_health_checks
        
        # Log to file
        echo "\$(date -Iseconds) - Health check completed" >> "\$LOG_DIR/health-check.log"
        
        # Wait for next check
        sleep \$CHECK_INTERVAL
    done
}

# Main function
case \$1 in
    "check")
        perform_health_checks
        ;;
    "monitor")
        continuous_monitoring
        ;;
    *)
        echo "Usage: \$0 [check|monitor]"
        exit 1
        ;;
esac
EOF
    
    # Make the script executable
    chmod +x "monitoring/scripts/health-check.sh"
    
    log "Health checks setup completed"
}

# Set up analytics tracking
setup_analytics_tracking() {
    log "Setting up analytics tracking..."
    
    # Create analytics configuration
    cat > "monitoring/analytics-config.json" << EOF
{
  "google_analytics": {
    "tracking_id": "UA-XXXXXXXXX-X",
    "property_name": "NormalDance Production",
    "custom_dimensions": {
      "user_type": "dimension1",
      "subscription_plan": "dimension2",
      "artist_status": "dimension3"
    },
    "goals": [
      {
        "name": "Signups",
        "type": "destination",
        "match_type": "exact",
        "url": "/signup"
      },
      {
        "name": "Track Uploads",
        "type": "destination",
        "match_type": "exact",
        "url": "/upload"
      },
      {
        "name": "Wallet Connections",
        "type": "destination",
        "match_type": "exact",
        "url": "/wallet"
      }
    ]
  },
  "mixpanel": {
    "token": "your_mixpanel_token",
    "events": [
      {
        "name": "User Signup",
        "properties": {
          "plan": "free",
          "source": "organic"
        }
      },
      {
        "name": "Track Upload",
        "properties": {
          "duration": 180,
          "genre": "electronic"
        }
      },
      {
        "name": "Wallet Connection",
        "properties": {
          "wallet_type": "phantom",
          "chain": "solana"
        }
      }
    ]
  },
  "amplitude": {
    "api_key": "your_amplitude_api_key",
    "events": [
      {
        "name": "Session Start",
        "properties": {
          "user_id": "{{user_id}}",
          "device_type": "{{device_type}}"
        }
      },
      {
        "name": "Track Play",
        "properties": {
          "track_id": "{{track_id}}",
          "artist_id": "{{artist_id}}",
          "duration": "{{duration}}"
        }
      }
    ]
  }
}
EOF
    
    # Create analytics tracking script
    cat > "monitoring/scripts/analytics-tracking.sh" << EOF
#!/bin/bash

# NormalDance Analytics Tracking Script
# This script handles analytics tracking and reporting

set -e

# Configuration
ANALYTICS_DIR="monitoring/analytics"
ANALYTICS_CONFIG="\$ANALYTICS_DIR/analytics-config.json"
LOG_DIR="monitoring/logs"
REPORT_DIR="monitoring/reports"

# Colors for output
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
RED='\033[0;31m'
NC='\033[0m'

log() {
    echo -e "\${GREEN}[INFO]\${NC} \$1"
}

warn() {
    echo -e "\${YELLOW}[WARN]\${NC} \$1"
}

error() {
    echo -e "\${RED}[ERROR]\${NC} \$1"
}

# Create directories
mkdir -p "\$LOG_DIR"
mkdir -p "\$REPORT_DIR"

# Function to track user signup
track_user_signup() {
    local user_id=\$1
    local email=\$2
    local plan=\$3
    local source=\$4
    
    log "Tracking user signup for \$user_id"
    
    # Track in Google Analytics
    curl -X POST "https://www.google-analytics.com/mp/collect" \\
        -H "Content-Type: application/json" \\
        -H "Authorization: Bearer \$GA_API_SECRET" \\
        -d "{
            \"client_id\": \"\$user_id\",
            \"events\": [{
                \"name\": \"sign_up\",
                \"params\": {
                    \"email\": \"\$email\",
                    \"plan\": \"\$plan\",
                    \"source\": \"\$source\"
                }
            }]
        }"
    
    # Track in Mixpanel
    curl -X POST "https://api.mixpanel.com/track" \\
        -H "Content-Type: application/json" \\
        -H "Authorization: Basic \$MIXPANEL_API_KEY" \\
        -d "{
            \"event\": \"User Signup\",
            \"properties\": {
                \"distinct_id\": \"\$user_id\",
                \"\$email\": \"\$email\",
                \"plan\": \"\$plan\",
                \"source\": \"\$source\"
            }
        }"
    
    # Track in Amplitude
    curl -X POST "https://api2.amplitude.com/2/httpapi" \\
        -H "Content-Type: application/json" \\
        -H "Authorization: Basic \$AMPLITUDE_API_KEY" \\
        -d "{
            \"api_key\": \"\$AMPLITUDE_API_KEY\",
            \"events\": [{
                \"user_id\": \"\$user_id\",
                \"event_type\": \"User Signup\",
                \"event_properties\": {
                    \"email\": \"\$email\",
                    \"plan\": \"\$plan\",
                    \"source\": \"\$source\"
                }
            }]
        }"
}

# Function to track track play
track_track_play() {
    local user_id=\$1
    local track_id=\$2
    local artist_id=\$3
    local duration=\$4
    
    log "Tracking track play for \$track_id by \$user_id"
    
    # Track in Google Analytics
    curl -X POST "https://www.google-analytics.com/mp/collect" \\
        -H "Content-Type: application/json" \\
        -H "Authorization: Bearer \$GA_API_SECRET" \\
        -d "{
            \"client_id\": \"\$user_id\",
            \"events\": [{
                \"name\": \"track_play\",
                \"params\": {
                    \"track_id\": \"\$track_id\",
                    \"artist_id\": \"\$artist_id\",
                    \"duration\": \"\$duration\"
                }
            }]
        }"
    
    # Track in Mixpanel
    curl -X POST "https://api.mixpanel.com/track" \\
        -H "Content-Type: application/json" \\
        -H "Authorization: Basic \$MIXPANEL_API_KEY" \\
        -d "{
            \"event\": \"Track Play\",
            \"properties\": {
                \"distinct_id\": \"\$user_id\",
                \"track_id\": \"\$track_id\",
                \"artist_id\": \"\$artist_id\",
                \"duration\": \"\$duration\"
            }
        }"
    
    # Track in Amplitude
    curl -X POST "https://api2.amplitude.com/2/httpapi" \\
        -H "Content-Type: application/json" \\
        -H "Authorization: Basic \$AMPLITUDE_API_KEY" \\
        -d "{
            \"api_key\": \"\$AMPLITUDE_API_KEY\",
            \"events\": [{
                \"user_id\": \"\$user_id\",
                \"event_type\": \"Track Play\",
                \"event_properties\": {
                    \"track_id\": \"\$track_id\",
                    \"artist_id\": \"\$artist_id\",
                    \"duration\": \"\$duration\"
                }
            }]
        }"
}

# Function to generate analytics report
generate_analytics_report() {
    local timestamp=\$(date +%Y%m%d_%H%M%S)
    local report_file="\$REPORT_DIR/analytics-report-\$timestamp.json"
    
    # Get analytics data from various sources
    local ga_data=\$(curl -s "https://www.google-analytics.com/data" \\
        -H "Authorization: Bearer \$GA_API_SECRET" \\
        -d "{
            \"metrics\": [
                {
                    \"name\": \"activeUsers\",
                    \"expression\": \"activeUsers\"
                }
            ],
            \"dimensions\": [
                {
                    \"name\": \"date\"
                }
            ]
        }")
    
    local mixpanel_data=\$(curl -s "https://api.mixpanel.com/export" \\
        -H "Authorization: Basic \$MIXPANEL_API_KEY" \\
        -d "{
            \"event\": \"[\\\"User Signup\\\", \\\"Track Play\\\"]\",
            \"from_date\": \"2024-01-01\",
            \"to_date\": \"2024-12-31\"
        }")
    
    local amplitude_data=\$(curl -s "https://api.amplitude.com/api/2/retention" \\
        -H "Content-Type: application/json" \\
        -H "Authorization: Basic \$AMPLITUDE_API_KEY" \\
        -d "{
            \"group_type\": \"user\",
            \"group_values\": [\"all\"],
            \"period\": \"day\",
            \"start\": \"2024-01-01\",
            \"end\": \"2024-12-31\"
        }")
    
    # Combine data into report
    cat > "\$report_file" << JSON
{
  "timestamp": "\$(date -Iseconds)",
  "google_analytics": \$ga_data,
  "mixpanel": \$mixpanel_data,
  "amplitude": \$amplitude_data,
  "summary": {
    "total_users": \$(curl -s "\$API_URL/users/count" | jq -r '.count'),
    "active_users_7d": \$(curl -s "\$API_URL/users/active?period=7d" | jq -r '.count'),
    "total_tracks": \$(curl -s "\$API_URL/tracks/count" | jq -r '.count'),
    "total_plays": \$(curl -s "\$API_URL/tracks/plays" | jq -r '.count'),
    "total_revenue": \$(curl -s "\$API_URL/revenue/total" | jq -r '.amount')
  }
}
JSON
    
    log "Analytics report generated: \$report_file"
}

# Main function
case \$1 in
    "track-signup")
        track_user_signup "\$2" "\$3" "\$4" "\$5"
        ;;
    "track-play")
        track_track_play "\$2" "\$3" "\$4" "\$5"
        ;;
    "report")
        generate_analytics_report
        ;;
    *)
        echo "Usage: \$0 [track-signup|track-play|report] [args...]"
        exit 1
        ;;
esac
EOF
    
    # Make the script executable
    chmod +x "monitoring/scripts/analytics-tracking.sh"
    
    log "Analytics tracking setup completed"
}

# Set up performance monitoring
setup_performance_monitoring() {
    log "Setting up performance monitoring..."
    
    # Create performance monitoring script
    cat > "monitoring/scripts/performance-monitoring.sh" << EOF
#!/bin/bash

# NormalDance Performance Monitoring Script
# This script monitors application performance

set -e

# Configuration
PERFORMANCE_DIR="monitoring/performance"
LOG_DIR="monitoring/logs"
API_URL="https://api.normaldance.com"
CHECK_INTERVAL=60
MAX_RESPONSE_TIME=2000
MAX_ERROR_RATE=0.01

# Colors for output
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
RED='\033[0;31m'
NC='\033[0m'

log() {
    echo -e "\${GREEN}[INFO]\${NC} \$1"
}

warn() {
    echo -e "\${YELLOW}[WARN]\${NC} \$1"
}

error() {
    echo -e "\${RED}[ERROR]\${NC} \$1"
}

# Create directories
mkdir -p "\$PERFORMANCE_DIR"
mkdir -p "\$LOG_DIR"

# Function to measure API response time
measure_api_response_time() {
    local endpoint=\$1
    local iterations=\${2:-10}
    local total_time=0
    
    for ((i=1; i<=\$iterations; i++)); do
        local start_time=\$(date +%s%N)
        curl -s "\$API_URL/\$endpoint" > /dev/null
        local end_time=\$(date +%s%N)
        local duration=\$((end_time - start_time))
        total_time=\$((total_time + duration))
    done
    
    local average_time=\$((total_time / iterations))
    echo "\$average_time"
}

# Function to measure error rate
measure_error_rate() {
    local endpoint=\$1
    local iterations=\${2:-100}
    local error_count=0
    
    for ((i=1; i<=\$iterations; i++)); do
        local response=\$(curl -s -o /dev/null -w "%{http_code}" "\$API_URL/\$endpoint")
        
        if [ "\$response" -ge 400 ]; then
            error_count=\$((error_count + 1))
        fi
    done
    
    local error_rate=\$(echo "scale=4; \$error_count / \$iterations" | bc)
    echo "\$error_rate"
}

# Function to measure database performance
measure_database_performance() {
    local query_time=\$(curl -s "\$API_URL/health/database" | jq -r '.query_time')
    local connection_count=\$(curl -s "\$API_URL/health/database" | jq -r '.connection_count')
    
    echo "Query Time: \$query_time ms, Connections: \$connection_count"
}

# Function to measure memory usage
measure_memory_usage() {
    local memory_usage=\$(ps -o pid,ppid,cmd,%mem,%cpu --sort=-%mem | head -6)
    echo "\$memory_usage"
}

# Function to measure CPU usage
measure_cpu_usage() {
    local cpu_usage=\$(top -bn1 | grep "Cpu(s)" | awk '{print \$2}' | cut -d'%' -f1)
    echo "\$cpu_usage"
}

# Function to generate performance report
generate_performance_report() {
    local timestamp=\$(date +%Y%m%d_%H%M%S)
    local report_file="\$PERFORMANCE_DIR/performance-report-\$timestamp.json"
    
    # Measure various performance metrics
    local api_response_time=\$(measure_api_response_time "health" 10)
    local error_rate=\$(measure_error_rate "health" 100)
    local db_performance=\$(measure_database_performance)
    local memory_usage=\$(measure_memory_usage)
    local cpu_usage=\$(measure_cpu_usage)
    
    # Create performance report
    cat > "\$report_file" << JSON
{
  "timestamp": "\$(date -Iseconds)",
  "api_performance": {
    "response_time_ms": \$api_response_time,
    "error_rate": \$error_rate,
    "status": "\$([ \"\$api_response_time\" -lt \"\$MAX_RESPONSE_TIME\" ] && echo \"healthy\" || echo \"degraded\")"
  },
  "database_performance": {
    "performance": "\$db_performance",
    "status": "\$([ \"\$db_performance\" =~ \"Query Time: [0-9]*\" ] && echo \"healthy\" || echo \"degraded\")"
  },
  "system_resources": {
    "memory_usage": "\$memory_usage",
    "cpu_usage": "\$cpu_usage",
    "status": "\$([ \"\$cpu_usage\" -lt \"80\" ] && echo \"healthy\" || echo \"degraded\")"
  },
  "recommendations": [
    "\$([ \"\$api_response_time\" -ge \"\$MAX_RESPONSE_TIME\" ] && echo \"Consider optimizing API endpoints\" || echo \"API performance is good\")",
    "\$([ \"\$error_rate\" -ge \"\$MAX_ERROR_RATE\" ] && echo \"Error rate is high, investigate API errors\" || echo \"Error rate is within acceptable limits\")",
    "\$([ \"\$cpu_usage\" -ge \"80\" ] && echo \"High CPU usage detected, consider scaling\" || echo \"CPU usage is normal\")"
  ]
}
JSON
    
    log "Performance report generated: \$report_file"
}

# Function to check performance thresholds
check_performance_thresholds() {
    local api_response_time=\$(measure_api_response_time "health" 10)
    local error_rate=\$(measure_error_rate "health" 100)
    local cpu_usage=\$(measure_cpu_usage)
    
    # Check API response time
    if [ "\$api_response_time" -ge "\$MAX_RESPONSE_TIME" ]; then
        warn "API response time is high: \$api_response_time ms"
    else
        log "API response time is good: \$api_response_time ms"
    fi
    
    # Check error rate
    if (( \$(echo "\$error_rate >= \$MAX_ERROR_RATE" | bc -l) )); then
        warn "Error rate is high: \$error_rate"
    else
        log "Error rate is good: \$error_rate"
    fi
    
    # Check CPU usage
    if [ "\$cpu_usage" -ge 80 ]; then
        warn "CPU usage is high: \$cpu_usage%"
    else
        log "CPU usage is good: \$cpu_usage%"
    fi
}

# Main function
case \$1 in
    "measure")
        generate_performance_report
        ;;
    "check")
        check_performance_thresholds
        ;;
    "monitor")
        while true; do
            check_performance_thresholds
            sleep \$CHECK_INTERVAL
        done
        ;;
    *)
        echo "Usage: \$0 [measure|check|monitor]"
        exit 1
        ;;
esac
EOF
    
    # Make the script executable
    chmod +x "monitoring/scripts/performance-monitoring.sh"
    
    log "Performance monitoring setup completed"
}

# Set up log aggregation
setup_log_aggregation() {
    log "Setting up log aggregation..."
    
    # Create log aggregation configuration
    cat > "monitoring/log-aggregation-config.yaml" << EOF
version: '3.8'

services:
  fluentd:
    image: fluent/fluentd:v1.16-1
    volumes:
      - ./fluentd/conf:/fluentd/etc
    ports:
      - "24224:24224"
      - "24224:24224/udp"
    environment:
      - FLUENTD_CONF=fluent.conf

  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.11.0
    environment:
      - discovery.type=single-node
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
    ports:
      - "9200:9200"
    volumes:
      - elasticsearch_data:/usr/share/elasticsearch/data

  kibana:
    image: docker.elastic.co/kibana/kibana:8.11.0
    ports:
      - "5601:5601"
    environment:
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
    depends_on:
      - elasticsearch

volumes:
  elasticsearch_data:
EOF
    
    # Create Fluentd configuration
    cat > "monitoring/fluentd/fluent.conf" << EOF
<source>
  @type forward
  port 24224
  bind 0.0.0.0
</source>

<match normaldance.**>
  @type elasticsearch
  host elasticsearch
  port 9200
  index_name normaldance-logs
  type_name _doc
  include_tag_key true
  tag_key @log_name
  <buffer>
    flush_mode interval
    flush_interval 5s
    flush_thread_count 8
    chunk_limit_size 2m
    queue_limit_length 32
    retry_max_interval 30
  </buffer>
</match>

<match **>
  @type stdout
</match>
EOF
    
    # Create log collection script
    cat > "monitoring/scripts/log-collection.sh" << EOF
#!/bin/bash

# NormalDance Log Collection Script
# This script collects and aggregates logs

set -e

# Configuration
LOG_DIR="monitoring/logs"
AGGREGATION_DIR="monitoring/aggregation"
DOCKER_COMPOSE_FILE="monitoring/log-aggregation-config.yaml"

# Colors for output
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
RED='\033[0;31m'
NC='\033[0m'

log() {
    echo -e "\${GREEN}[INFO]\${NC} \$1"
}

warn() {
    echo -e "\${YELLOW}[WARN]\${NC} \$1"
}

error() {
    echo -e "\${RED}[ERROR]\${NC} \$1"
}

# Create directories
mkdir -p "\$LOG_DIR"
mkdir -p "\$AGGREGATION_DIR"

# Function to start log aggregation
start_log_aggregation() {
    log "Starting log aggregation..."
    
    # Start Docker containers
    docker-compose -f "\$DOCKER_COMPOSE_FILE" up -d
    
    log "Log aggregation started"
}

# Function to stop log aggregation
stop_log_aggregation() {
    log "Stopping log aggregation..."
    
    # Stop Docker containers
    docker-compose -f "\$DOCKER_COMPOSE_FILE" down
    
    log "Log aggregation stopped"
}

# Function to collect logs
collect_logs() {
    local timestamp=\$(date +%Y%m%d_%H%M%S)
    local log_file="\$LOG_DIR/application-\$timestamp.log"
    
    # Collect application logs
    docker logs normaldance-api > "\$log_file" 2>&1
    
    # Collect system logs
    journalctl -u normaldance-api > "\$LOG_DIR/system-\$timestamp.log" 2>&1
    
    # Collect access logs
    tail -n 1000 /var/log/nginx/access.log > "\$LOG_DIR/access-\$timestamp.log" 2>&1
    
    log "Logs collected to \$log_file"
}

# Function to search logs
search_logs() {
    local query=\$1
    local log_type=\${2:-"all"}
    
    case \$log_type in
        "api")
            grep -r "\$query" "\$LOG_DIR"/*api*.log
            ;;
        "system")
            grep -r "\$query" "\$LOG_DIR"/*system*.log
            ;;
        "access")
            grep -r "\$query" "\$LOG_DIR"/*access*.log
            ;;
        *)
            grep -r "\$query" "\$LOG_DIR"/*.log
            ;;
    esac
}

# Function to generate log report
generate_log_report() {
    local timestamp=\$(date +%Y%m%d_%H%M%S)
    local report_file="\$AGGREGATION_DIR/log-report-\$timestamp.json"
    
    # Count log entries by type
    local api_logs=\$(find "\$LOG_DIR" -name "*api*.log" | wc -l)
    local system_logs=\$(find "\$LOG_DIR" -name "*system*.log" | wc -l)
    local access_logs=\$(find "\$LOG_DIR" -name "*access*.log" | wc -l)
    
    # Count error logs
    local error_logs=\$(grep -r "ERROR" "\$LOG_DIR"/*.log | wc -l)
    local warning_logs=\$(grep -r "WARN" "\$LOG_DIR"/*.log | wc -l)
    
    # Create log report
    cat > "\$report_file" << JSON
{
  "timestamp": "\$(date -Iseconds)",
  "log_counts": {
    "api_logs": \$api_logs,
    "system_logs": \$system_logs,
    "access_logs": \$access_logs
  },
  "error_counts": {
    "errors": \$error_logs,
    "warnings": \$warning_logs
  },
  "log_sizes": {
    "total_size": \$(du -sh "\$LOG_DIR" | cut -f1),
    "api_size": \$(du -sh "\$LOG_DIR"/*api*.log 2>/dev/null | cut -f1 || echo "0"),
    "system_size": \$(du -sh "\$LOG_DIR"/*system*.log 2>/dev/null | cut -f1 || echo "0"),
    "access_size": \$(du -sh "\$LOG_DIR"/*access*.log 2>/dev/null | cut -f1 || echo "0")
  },
  "recent_errors": [
    \$(grep -r "ERROR" "\$LOG_DIR"/*.log | tail -5 | jq -R -s -c '.')
  ]
}
JSON
    
    log "Log report generated: \$report_file"
}

# Main function
case \$1 in
    "start")
        start_log_aggregation
        ;;
    "stop")
        stop_log_aggregation
        ;;
    "collect")
        collect_logs
        ;;
    "search")
        search_logs "\$2" "\$3"
        ;;
    "report")
        generate_log_report
        ;;
    *)
        echo "Usage: \$0 [start|stop|collect|search|report] [args...]"
        exit 1
        ;;
esac
EOF
    
    # Make the script executable
    chmod +x "monitoring/scripts/log-collection.sh"
    
    log "Log aggregation setup completed"
}

# Set up backup and disaster recovery
setup_backup_disaster_recovery() {
    log "Setting up backup and disaster recovery..."
    
    # Create backup configuration
    cat > "monitoring/backup-config.yaml" << EOF
version: '3.8'

services:
  postgres-backup:
    image: postgres:15
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./backups:/backups
    environment:
      - POSTGRES_DB=normaldance
      - POSTGRES_USER=normaldance
      - POSTGRES_PASSWORD=\${POSTGRES_PASSWORD}
    command: >
      bash -c "
        while true; do
          pg_dump -U normaldance normaldance > /backups/postgres-\$(date +%Y%m%d_%H%M%S).sql
          sleep 86400
        done
      "

  redis-backup:
    image: redis:7
    volumes:
      - redis_data:/data
      - ./backups:/backups
    command: >
      bash -c "
        while true; do
          redis-cli --rdb /backups/redis-\$(date +%Y%m%d_%H%M%S).rdb
          sleep 86400
        done
      "

  file-backup:
    image: alpine:latest
    volumes:
      - ./backups:/backups
      - ./uploads:/uploads
    command: >
      bash -c "
        while true; do
          tar -czf /backups/files-\$(date +%Y%m%d_%H%M%S).tar.gz -C /uploads .
          sleep 86400
        done
      "

volumes:
  postgres_data:
  redis_data:
EOF
    
    # Create backup script
    cat > "monitoring/scripts/backup.sh" << EOF
#!/bin/bash

# NormalDance Backup Script
# This script handles database and file backups

set -e

# Configuration
BACKUP_DIR="monitoring/backups"
RETENTION_DAYS=30
AWS_S3_BUCKET="s3://normaldance-backups"
AWS_ACCESS_KEY_ID="\${AWS_ACCESS_KEY_ID}"
AWS_SECRET_ACCESS_KEY="\${AWS_SECRET_ACCESS_KEY}"
POSTGRES_HOST="\${POSTGRES_HOST:-localhost}"
POSTGRES_USER="\${POSTGRES_USER:-normaldance}"
POSTGRES_PASSWORD="\${POSTGRES_PASSWORD}"
POSTGRES_DB="\${POSTGRES_DB:-normaldance}"
REDIS_HOST="\${REDIS_HOST:-localhost}"
UPLOADS_DIR="\${UPLOADS_DIR:-./uploads}"

# Colors for output
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
RED='\033[0;31m'
NC='\033[0m'

log() {
    echo -e "\${GREEN}[INFO]\${NC} \$1"
}

warn() {
    echo -e "\${YELLOW}[WARN]\${NC} \$1"
}

error() {
    echo -e "\${RED}[ERROR]\${NC} \$1"
}

# Create backup directory
mkdir -p "\$BACKUP_DIR"

# Function to backup PostgreSQL
backup_postgres() {
    local timestamp=\$(date +%Y%m%d_%H%M%S)
    local backup_file="\$BACKUP_DIR/postgres-\$timestamp.sql"
    
    log "Starting PostgreSQL backup..."
    
    # Create backup
    PGPASSWORD="\$POSTGRES_PASSWORD" pg_dump -h "\$POSTGRES_HOST" -U "\$POSTGRES_USER" "\$POSTGRES_DB" > "\$backup_file"
    
    # Compress backup
    gzip "\$backup_file"
    
    # Upload to S3
    aws s3 cp "\$backup_file.gz" "\$AWS_S3_BUCKET/postgres/" --acl bucket-owner-full-control
    
    log "PostgreSQL backup completed: \$backup_file.gz"
}

# Function to backup Redis
backup_redis() {
    local timestamp=\$(date +%Y%m%d_%H%M%S)
    local backup_file="\$BACKUP_DIR/redis-\$timestamp.rdb"
    
    log "Starting Redis backup..."
    
    # Create backup
    redis-cli -h "\$REDIS_HOST" BGSAVE
    
    # Copy backup file
    cp "/var/lib/redis/dump.rdb" "\$backup_file"
    
    # Compress backup
    gzip "\$backup_file"
    
    # Upload to S3
    aws s3 cp "\$backup_file.gz" "\$AWS_S3_BUCKET/redis/" --acl bucket-owner-full-control
    
    log "Redis backup completed: \$backup_file.gz"
}

# Function to backup files
backup_files() {
    local timestamp=\$(date +%Y%m%d_%H%M%S)
    local backup_file="\$BACKUP_DIR/files-\$timestamp.tar.gz"
    
    log "Starting files backup..."
    
    # Create backup
    tar -czf "\$backup_file" -C "\$UPLOADS_DIR" .
    
    # Upload to S3
    aws s3 cp "\$backup_file" "\$AWS_S3_BUCKET/files/" --acl bucket-owner-full-control
    
    log "Files backup completed: \$backup_file"
}

# Function to cleanup old backups
cleanup_old_backups() {
    log "Cleaning up old backups..."
    
    # Find and remove backups older than retention period
    find "\$BACKUP_DIR" -name "*.sql.gz" -mtime +\$RETENTION_DAYS -delete
    find "\$BACKUP_DIR" -name "*.rdb.gz" -mtime +\$RETENTION_DAYS -delete
    find "\$BACKUP_DIR" -name "*.tar.gz" -mtime +\$RETENTION_DAYS -delete
    
    # Cleanup S3
    aws s3 ls "\$AWS_S3_BUCKET/" --recursive | while read -r line; do
        createDate=\$(echo \$line | awk '{print $1" "$2}')
        createDateTS=\$(date -d "\$createDate" +%s)
        olderThanTS=\$(date -d "\$RETENTION_DAYS days ago" +%s)
        
        if [ \$createDateTS -lt \$olderThanTS ]; then
            fileName=\$(echo \$line | awk '{print $4}')
            aws s3 rm "\$AWS_S3_BUCKET/\$fileName"
        fi
    done
    
    log "Cleanup completed"
}

# Function to restore from backup
restore_backup() {
    local backup_type=\$1
    local backup_file=\$2
    
    if [ ! -f "\$backup_file" ]; then
        error "Backup file not found: \$backup_file"
    fi
    
    case \$backup_type in
        "postgres")
            log "Restoring PostgreSQL from \$backup_file"
            gunzip -c "\$backup_file" | psql -h "\$POSTGRES_HOST" -U "\$POSTGRES_USER" "\$POSTGRES_DB"
            ;;
        "redis")
            log "Restoring Redis from \$backup_file"
            gunzip -c "\$backup_file" > /var/lib/redis/dump.rdb
            redis-cli -h "\$REDIS_HOST" BGSAVE
            ;;
        "files")
            log "Restoring files from \$backup_file"
            tar -xzf "\$backup_file" -C "\$UPLOADS_DIR"
            ;;
        *)
            error "Unknown backup type: \$backup_type"
            ;;
    esac
    
    log "Restore completed"
}

# Main function
case \$1 in
    "postgres")
        backup_postgres
        ;;
    "redis")
        backup_redis
        ;;
    "files")
        backup_files
        ;;
    "cleanup")
        cleanup_old_backups
        ;;
    "restore")
        restore_backup "\$2" "\$3"
        ;;
    "all")
        backup_postgres
        backup_redis
        backup_files
        cleanup_old_backups
        ;;
    *)
        echo "Usage: \$0 [postgres|redis|files|cleanup|restore|all] [backup_file]"
        exit 1
        ;;
esac
EOF
    
    # Make the script executable
    chmod +x "monitoring/scripts/backup.sh"
    
    log "Backup and disaster recovery setup completed"
}

# Main function
main() {
    log "Starting NormalDance launch and monitoring setup..."
    
    # Check prerequisites
    check_prerequisites
    
    # Create monitoring directory
    create_monitoring_directory
    
    # Set up monitoring infrastructure
    setup_monitoring_infrastructure
    
    # Set up health checks
    setup_health_checks
    
    # Set up analytics tracking
    setup_analytics_tracking
    
    # Set up performance monitoring
    setup_performance_monitoring
    
    # Set up log aggregation
    setup_log_aggregation
    
    # Set up backup and disaster recovery
    setup_backup_disaster_recovery
    
    log "NormalDance launch and monitoring setup completed successfully!"
    
    # Display next steps
    echo ""
    echo "Next Steps:"
    echo "1. Configure monitoring tools with your credentials"
    echo "2. Start monitoring services: ./monitoring/scripts/health-check.sh monitor"
    echo "3. Set up alerts and notifications"
    echo "4. Configure backup automation"
    echo "5. Test disaster recovery procedures"
    echo ""
    echo "Monitoring Dashboard: \$GRAFANA_URL"
    echo "Alert Manager: \$SLACK_WEBHOOK"
    echo "Backup Location: \$AWS_S3_BUCKET"
}

# Run main function
main "$@"